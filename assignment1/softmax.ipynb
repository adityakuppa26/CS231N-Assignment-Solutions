{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"softmax.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"1RhhCkX5YsqD","executionInfo":{"status":"ok","timestamp":1601253426488,"user_tz":-330,"elapsed":42049,"user":{"displayName":"aditya kuppa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghxy57ebNGX26QRkbLIfA4tHHTCXTiPAVAfiRQaUw=s64","userId":"14605460428474082045"}},"outputId":"8d302a4e-f2f9-4698-e891-bd87f454e8ac","colab":{"base_uri":"https://localhost:8080/","height":462}},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# 'cs231n' folder containing the '.py', 'classifiers' and 'datasets'\n","# folders.\n","# e.g. 'cs231n/assignments/assignment1/cs231n/'\n","FOLDERNAME = 'CS231n/assignment1/cs231n/'\n","\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","%cd drive/My\\ Drive\n","%cp -r $FOLDERNAME ../../\n","%cd ../../\n","%cd cs231n/datasets/\n","!bash get_datasets.sh\n","%cd ../../"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive\n","/content\n","/content/cs231n/datasets\n","--2020-09-28 00:36:55--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170498071 (163M) [application/x-gzip]\n","Saving to: ‘cifar-10-python.tar.gz’\n","\n","cifar-10-python.tar 100%[===================>] 162.60M  18.8MB/s    in 7.7s    \n","\n","2020-09-28 00:37:02 (21.1 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n","\n","cifar-10-batches-py/\n","cifar-10-batches-py/data_batch_4\n","cifar-10-batches-py/readme.html\n","cifar-10-batches-py/test_batch\n","cifar-10-batches-py/data_batch_3\n","cifar-10-batches-py/batches.meta\n","cifar-10-batches-py/data_batch_2\n","cifar-10-batches-py/data_batch_5\n","cifar-10-batches-py/data_batch_1\n","/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BuzB37xPJKPW","executionInfo":{"status":"ok","timestamp":1601253454619,"user_tz":-330,"elapsed":1031,"user":{"displayName":"aditya kuppa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghxy57ebNGX26QRkbLIfA4tHHTCXTiPAVAfiRQaUw=s64","userId":"14605460428474082045"}},"outputId":"8caeda1a-1bf8-4824-ee7a-8eac7aeb303d","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"tags":["pdf-title"],"id":"yOXGn3ToYsqN"},"source":["# Softmax exercise\n","\n","*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n","\n","This exercise is analogous to the SVM exercise. You will:\n","\n","- implement a fully-vectorized **loss function** for the Softmax classifier\n","- implement the fully-vectorized expression for its **analytic gradient**\n","- **check your implementation** with numerical gradient\n","- use a validation set to **tune the learning rate and regularization** strength\n","- **optimize** the loss function with **SGD**\n","- **visualize** the final learned weights\n"]},{"cell_type":"code","metadata":{"tags":["pdf-ignore"],"id":"08EZ0TpfYsqO"},"source":["import random\n","import numpy as np\n","from cs231n.data_utils import load_CIFAR10\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading extenrnal modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":["pdf-ignore"],"id":"K7V7SV-JYsqU","executionInfo":{"status":"ok","timestamp":1601191471752,"user_tz":-330,"elapsed":4550,"user":{"displayName":"aditya kuppa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghxy57ebNGX26QRkbLIfA4tHHTCXTiPAVAfiRQaUw=s64","userId":"14605460428474082045"}},"outputId":"a568628e-7019-4b67-fb6e-b99c78ea5ba5","colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n","    \"\"\"\n","    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n","    it for the linear classifier. These are the same steps as we used for the\n","    SVM, but condensed to a single function.  \n","    \"\"\"\n","    # Load the raw CIFAR-10 data\n","    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n","    \n","    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n","    try:\n","       del X_train, y_train\n","       del X_test, y_test\n","       print('Clear previously loaded data.')\n","    except:\n","       pass\n","\n","    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n","    \n","    # subsample the data\n","    mask = list(range(num_training, num_training + num_validation))\n","    X_val = X_train[mask]\n","    y_val = y_train[mask]\n","    mask = list(range(num_training))\n","    X_train = X_train[mask]\n","    y_train = y_train[mask]\n","    mask = list(range(num_test))\n","    X_test = X_test[mask]\n","    y_test = y_test[mask]\n","    mask = np.random.choice(num_training, num_dev, replace=False)\n","    X_dev = X_train[mask]\n","    y_dev = y_train[mask]\n","    \n","    # Preprocessing: reshape the image data into rows\n","    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n","    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n","    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n","    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n","    \n","    # Normalize the data: subtract the mean image\n","    mean_image = np.mean(X_train, axis = 0)\n","    X_train -= mean_image\n","    X_val -= mean_image\n","    X_test -= mean_image\n","    X_dev -= mean_image\n","    \n","    # add bias dimension and transform into columns\n","    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n","    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n","    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n","    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n","    \n","    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n","\n","\n","# Invoke the above function to get our data.\n","X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n","print('Train data shape: ', X_train.shape)\n","print('Train labels shape: ', y_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Validation labels shape: ', y_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)\n","print('dev data shape: ', X_dev.shape)\n","print('dev labels shape: ', y_dev.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train data shape:  (49000, 3073)\n","Train labels shape:  (49000,)\n","Validation data shape:  (1000, 3073)\n","Validation labels shape:  (1000,)\n","Test data shape:  (1000, 3073)\n","Test labels shape:  (1000,)\n","dev data shape:  (500, 3073)\n","dev labels shape:  (500,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e6Cf3SCAYsqZ"},"source":["## Softmax Classifier\n","\n","Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"]},{"cell_type":"code","metadata":{"id":"4VVvZNW5Ysqa","executionInfo":{"status":"ok","timestamp":1601200794976,"user_tz":-330,"elapsed":1114,"user":{"displayName":"aditya kuppa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghxy57ebNGX26QRkbLIfA4tHHTCXTiPAVAfiRQaUw=s64","userId":"14605460428474082045"}},"outputId":"9ac48f61-3de7-4900-cac4-c8f82b56eba5","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# First implement the naive softmax loss function with nested loops.\n","# Open the file cs231n/classifiers/softmax.py and implement the\n","# softmax_loss_naive function.\n","\n","from cs231n.classifiers.softmax import softmax_loss_naive\n","import time\n","\n","# Generate a random softmax weight matrix and use it to compute the loss.\n","W = np.random.randn(3073, 10) * 0.0001\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n","\n","# As a rough sanity check, our loss should be something close to -log(0.1).\n","print('loss: %f' % loss)\n","print('sanity check: %f' % (-np.log(0.1)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loss: 2.319438\n","sanity check: 2.302585\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"tags":["pdf-inline"],"id":"ypqVHKYnYsqe"},"source":["**Inline Question 1**\n","\n","Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n","\n","$\\color{blue}{\\textit Your Answer:}$ *Since the weights are initialized randomly, and there is no parameter update i.e no training, all the classes should be equally likely to occur. SInce there are 10 classes in CIFAR-10, each class would then have 0.1 probability. As the loss for softmax is a negative log of prediction, the loss is close to -log(0.1)* \n","\n"]},{"cell_type":"code","metadata":{"id":"t-3h3zBQYsqf","executionInfo":{"status":"ok","timestamp":1601200804249,"user_tz":-330,"elapsed":5173,"user":{"displayName":"aditya kuppa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghxy57ebNGX26QRkbLIfA4tHHTCXTiPAVAfiRQaUw=s64","userId":"14605460428474082045"}},"outputId":"e3c7054d-bb60-45b8-875a-6e68b8943ec7","colab":{"base_uri":"https://localhost:8080/","height":353}},"source":["# Complete the implementation of softmax_loss_naive and implement a (naive)\n","# version of the gradient that uses nested loops.\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n","\n","# As we did for the SVM, use numeric gradient checking as a debugging tool.\n","# The numeric gradient should be close to the analytic gradient.\n","from cs231n.gradient_check import grad_check_sparse\n","f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n","grad_numerical = grad_check_sparse(f, W, grad, 10)\n","\n","# similar to SVM case, do another gradient check with regularization\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n","f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n","grad_numerical = grad_check_sparse(f, W, grad, 10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["numerical: -0.705970 analytic: -0.705970, relative error: 4.439905e-08\n","numerical: 1.119160 analytic: 1.119160, relative error: 3.181034e-08\n","numerical: 1.987568 analytic: 1.987568, relative error: 4.278732e-08\n","numerical: -0.465732 analytic: -0.465732, relative error: 2.239838e-08\n","numerical: 1.429016 analytic: 1.429016, relative error: 3.533853e-08\n","numerical: -0.517028 analytic: -0.517028, relative error: 7.189673e-08\n","numerical: 1.822438 analytic: 1.822438, relative error: 2.955700e-08\n","numerical: -0.293906 analytic: -0.293906, relative error: 1.144908e-07\n","numerical: -1.563549 analytic: -1.563548, relative error: 1.818816e-08\n","numerical: 1.283411 analytic: 1.283411, relative error: 5.825690e-08\n","numerical: -2.610387 analytic: -2.610387, relative error: 1.624145e-08\n","numerical: 3.081723 analytic: 3.081723, relative error: 2.533642e-09\n","numerical: -0.700739 analytic: -0.700739, relative error: 6.881674e-08\n","numerical: 2.010369 analytic: 2.010369, relative error: 7.653642e-09\n","numerical: 2.236855 analytic: 2.236855, relative error: 1.033804e-08\n","numerical: -0.723497 analytic: -0.723497, relative error: 6.871788e-09\n","numerical: -2.850369 analytic: -2.850369, relative error: 2.128210e-08\n","numerical: -0.968044 analytic: -0.968044, relative error: 9.783218e-08\n","numerical: -0.099990 analytic: -0.099990, relative error: 3.452699e-07\n","numerical: -2.239042 analytic: -2.239042, relative error: 6.403984e-09\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PbWjvzbiYsqj","executionInfo":{"status":"ok","timestamp":1601200808285,"user_tz":-330,"elapsed":1675,"user":{"displayName":"aditya kuppa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghxy57ebNGX26QRkbLIfA4tHHTCXTiPAVAfiRQaUw=s64","userId":"14605460428474082045"}},"outputId":"6ab74bd0-03fd-4424-a895-f20df4b60271","colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["# Now that we have a naive implementation of the softmax loss function and its gradient,\n","# implement a vectorized version in softmax_loss_vectorized.\n","# The two versions should compute the same results, but the vectorized version should be\n","# much faster.\n","tic = time.time()\n","loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n","toc = time.time()\n","print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n","\n","from cs231n.classifiers.softmax import softmax_loss_vectorized\n","tic = time.time()\n","loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n","toc = time.time()\n","print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n","\n","# As we did for the SVM, we use the Frobenius norm to compare the two versions\n","# of the gradient.\n","grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n","print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n","print('Gradient difference: %f' % grad_difference)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["naive loss: 2.319438e+00 computed in 0.064445s\n","vectorized loss: 2.319438e+00 computed in 0.010708s\n","Loss difference: 0.000000\n","Gradient difference: 0.000000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tuning","tags":["code"]},"source":["# Use the validation set to tune hyperparameters (regularization strength and\n","# learning rate). You should experiment with different ranges for the learning\n","# rates and regularization strengths; if you are careful you should be able to\n","# get a classification accuracy of over 0.35 on the validation set.\n","\n","from cs231n.classifiers import Softmax\n","results = {}\n","best_val = -1\n","best_softmax = None\n","\n","################################################################################\n","# TODO:                                                                        #\n","# Use the validation set to set the learning rate and regularization strength. #\n","# This should be identical to the validation that you did for the SVM; save    #\n","# the best trained softmax classifer in best_softmax.                          #\n","################################################################################\n","\n","# Provided as a reference. You may or may not want to change these hyperparameters\n","learning_rates = [1e-7, 5e-7]\n","regularization_strengths = [2.5e4, 5e4]\n","\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","def search_random_hyperparameters(lr, rs):\n","  ler = lr[np.random.randint(0, len(lr))]\n","  regs = rs[np.random.randint(0, len(rs))]\n","  return ler, regs\n","\n","for i in range(10):\n","  lr, rg = search_random_hyperparameters(learning_rates, regularization_strengths)\n","  model = Softmax()\n","  model.train(X_train, y_train, learning_rate=lr, reg=rg, num_iters=1000)\n","  y_train_pred = model.predict(X_train)\n","  train_acc = np.mean(y_train_pred == y_train)\n","  y_val_pred = model.predict(X_val)\n","  val_acc = np.mean(y_val_pred == y_val)\n","\n","  results[(lr, rg)] = (train_acc, val_acc)\n","  if val_acc > best_val:\n","    best_val = val_acc\n","    best_softmax = model\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    \n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n","                lr, reg, train_accuracy, val_accuracy))\n","    \n","print('best validation accuracy achieved during cross-validation: %f' % best_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"test"},"source":["# evaluate on test set\n","# Evaluate the best softmax on test set\n","y_test_pred = best_softmax.predict(X_test)\n","test_accuracy = np.mean(y_test == y_test_pred)\n","print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":["pdf-inline"],"id":"RZC1o5RJYsqy"},"source":["**Inline Question 2** - *True or False*\n","\n","Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n","\n","$\\color{blue}{\\textit Your Answer:}$\n","\n","\n","$\\color{blue}{\\textit Your Explanation:}$\n","\n"]},{"cell_type":"code","metadata":{"id":"4STzvO8sYsqy"},"source":["# Visualize the learned weights for each class\n","w = best_softmax.W[:-1,:] # strip out the bias\n","w = w.reshape(32, 32, 3, 10)\n","\n","w_min, w_max = np.min(w), np.max(w)\n","\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","for i in range(10):\n","    plt.subplot(2, 5, i + 1)\n","    \n","    # Rescale the weights to be between 0 and 255\n","    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n","    plt.imshow(wimg.astype('uint8'))\n","    plt.axis('off')\n","    plt.title(classes[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sIMApl9PYsq3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7uKiqE6nYsq8"},"source":["---\n","# IMPORTANT\n","\n","This is the end of this question. Please do the following:\n","\n","1. Click `File -> Save` to make sure the latest checkpoint of this notebook is saved to your Drive.\n","2. Execute the cell below to download the modified `.py` files back to your drive."]},{"cell_type":"code","metadata":{"id":"_oALhZLCYsq9"},"source":["import os\n","\n","FOLDER_TO_SAVE = os.path.join('drive/My Drive/', FOLDERNAME)\n","FILES_TO_SAVE = ['cs231n/classifiers/softmax.py']\n","\n","for files in FILES_TO_SAVE:\n","  with open(os.path.join(FOLDER_TO_SAVE, '/'.join(files.split('/')[1:])), 'w') as f:\n","    f.write(''.join(open(files).readlines()))"],"execution_count":null,"outputs":[]}]}